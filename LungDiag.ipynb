{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from scipy import interp\n",
    "import numpy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取所有的json文件名\n",
    "file_path = '../train_data/'\n",
    "\n",
    "FilePath_list = []\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for file in files:\n",
    "        FilePath    = os.path.join(root, file)\n",
    "        FilePath_list.append(FilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_feature_name_file = 'clinical_features_synonym.xlsx'\n",
    "df_cf = pd.read_excel(clinical_feature_name_file)\n",
    "\n",
    "\n",
    "clinical_feature_list = []\n",
    "for i in df_cf[\"临床特征\"].values.tolist():\n",
    "    clinical_feature_list.append(i)\n",
    "\n",
    "\n",
    "# 首先获得整个列名\n",
    "# 也就是把所有变量名称全都加上属性“发热_存在情况”的形式\n",
    "\n",
    "ssu_attributes = ['存在情况', '严重程度', '持续时间', '痛觉性质', '发作模式', '加剧因素', '缓解因素', '分布模式', '偏侧性', '四分区']\n",
    "\n",
    "disease_symptom_attr_list = []\n",
    "\n",
    "\n",
    "# 构建加上属性的表头\n",
    "for i in clinical_feature_list:\n",
    "    for ssu_attribute in ssu_attributes:\n",
    "        disease_symptom_attr = i + '_' + ssu_attribute\n",
    "        disease_symptom_attr_list.append(disease_symptom_attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建临床特征的同义词字典\n",
    "term_synonym_list = []\n",
    "for synonym in df_cf[\"同义词\"].values.tolist():\n",
    "    term_synonym = synonym.split('||')\n",
    "    term_synonym_list.append(term_synonym)\n",
    "term_synonym_dict = dict(zip(clinical_feature_list, term_synonym_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建“疾病名称”的归一化字典\n",
    "disease_classification_file = 'disease_classification_name.xlsx'\n",
    "df_classification = pd.read_excel(disease_classification_file)\n",
    "# df_classification = pd.read_excel(disease_classification_file,sheet_name='慢阻肺')\n",
    "\n",
    "Classification_group_list = []\n",
    "for i in df_classification[\"Classification group\"].values.tolist():\n",
    "    Classification_group_list.append(i)\n",
    "# print(Classification_group_list)\n",
    "\n",
    "\n",
    "\n",
    "disease_classification_list = []\n",
    "for synonym in df_classification[\"disease_name\"].values.tolist():\n",
    "    disease_synonym = synonym.split('||')\n",
    "    disease_classification_list.append(disease_synonym)\n",
    "disease_classification_dict = dict(zip(Classification_group_list, disease_classification_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_symptom_clinical_all = []\n",
    "disease_Inpatient_number_all = []\n",
    "for file_name in FilePath_list:\n",
    "    with open(file_name,'r',encoding='utf-8') as load_f:\n",
    "        term_ann_list =  []\n",
    "        disease_name = file_name.split('train_data/')[1].split('_')[0]\n",
    "        Inpatient_number = file_name.split('train_data/')[1].split('_')[1].split('.json')[0]\n",
    "        disease_Inpatient_number = disease_name + '_' + Inpatient_number\n",
    "        disease_Inpatient_number_all.append(disease_Inpatient_number)\n",
    "        for key_d,value_d in disease_classification_dict.items():\n",
    "            if disease_name in value_d:\n",
    "                load_dict = json.load(load_f)\n",
    "                annot_infos = load_dict['标注信息']\n",
    "                for annot_info in annot_infos.values():\n",
    "                    term_ann_type = annot_info[0]['术语类型']\n",
    "                    for ssu_attribute in ssu_attributes:\n",
    "                        if term_ann_type in ['症状','疾病','影像学表现'] and annot_info[1]['存在情况']['标准取值'] != '否认':\n",
    "                            term_ann_notlab = annot_info[0]['术语名称']\n",
    "                            term_attr = annot_info[1][ssu_attribute]['标准取值']\n",
    "                            for key_s,value_s in term_synonym_dict.items():\n",
    "                                if term_ann_notlab in value_s:\n",
    "                                    term_ann_list.append([key_s+'_'+ssu_attribute,term_attr])\n",
    "\n",
    "\n",
    "                        if term_ann_type == '定量定性检查':\n",
    "                            term_ann_lab =  annot_info[0]['术语名称'] + annot_info[1]['结果判读']['标准取值']\n",
    "                            term_attr_lab = annot_info[1][ssu_attribute]['标准取值']\n",
    "                            for key_s,value_s in term_synonym_dict.items():\n",
    "                                if term_ann_lab in value_s:\n",
    "                                    term_ann_list.append([key_s+'_'+ssu_attribute,term_attr_lab])\n",
    "\n",
    "                        if term_ann_type in ['药物','手术'] and annot_info[1]['存在情况']['标准取值'] != '否认':\n",
    "                            term_ann_treat =  annot_info[0]['术语名称']\n",
    "                            term_treat_attr = annot_info[1]['存在情况']['标准取值']\n",
    "                            for key_s,value_s in term_synonym_dict.items():\n",
    "                                if term_ann_treat in value_s:\n",
    "                                    term_ann_list.append([key_s+'_'+'存在情况',term_treat_attr])\n",
    "\n",
    "\n",
    "                list_fir_symptom = []\n",
    "                term_ann_first_dict = {}\n",
    "                for i in range(len(term_ann_list)):\n",
    "                    if term_ann_list[i][0] not in list_fir_symptom:\n",
    "                        # print(term_ann_list[i])\n",
    "                        list_fir_symptom.append(term_ann_list[i][0])\n",
    "                        term_ann_first_dict[term_ann_list[i][0]] = term_ann_list[i][1]\n",
    "                # print(term_ann_first_dict)\n",
    "\n",
    "                res = []\n",
    "                for x in disease_symptom_attr_list:\n",
    "                    if x in term_ann_first_dict.keys():\n",
    "                        res.append(term_ann_first_dict[x])\n",
    "                    else:\n",
    "                        res.append('NULL')\n",
    "                disease_symptom_clinical_all.append([key_d,res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_ann_list = []\n",
    "for file_name in FilePath_list:\n",
    "    # print(file_name)\n",
    "    with open(file_name,'r',encoding='utf-8') as load_f:\n",
    "        load_dict = json.load(load_f)\n",
    "        annot_infos = load_dict['标注信息']\n",
    "        for annot_info in annot_infos.values():\n",
    "            term_ann_type = annot_info[0]['术语类型']\n",
    "            if term_ann_type in ['症状','疾病','影像学表现'] and annot_info[1]['存在情况']['标准取值'] != '否认':\n",
    "                term_ann = annot_info[0]['术语名称']\n",
    "                term_loca = annot_info[1]['发作部位']\n",
    "                # 没有部位的直接把术语写进去\n",
    "                term_ann_list.append(term_ann)\n",
    "            if term_ann_type == '定量定性检查':\n",
    "                term_ann_labtest = annot_info[0]['术语名称'] + annot_info[1]['结果判读']['标准取值']\n",
    "                term_ann_list.append(term_ann_labtest)\n",
    "                \n",
    "            if term_ann_type in ['药物','手术'] and annot_info[1]['存在情况']['标准取值'] != '否认':\n",
    "                term_ann_treat =  annot_info[0]['术语名称']\n",
    "                term_ann_list.append(term_ann_treat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_age = []\n",
    "patient_gender = []\n",
    "for file_name in FilePath_list:\n",
    "    with open(file_name,'r',encoding='utf-8') as load_f:\n",
    "        load_dict = json.load(load_f)\n",
    "        patient_age_gender =load_dict['原文信息'].split('。')[0].split('岁,')\n",
    "\n",
    "        patient_age.append(int(patient_age_gender[0]))\n",
    "        patient_gender.append(patient_age_gender[1].replace('男','1').replace('女','0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "disease_name = []\n",
    "disease_name_1 = []\n",
    "for disease_symptom_list_ in disease_symptom_clinical_all:\n",
    "    df_list.append(disease_symptom_list_[1])\n",
    "    disease_name_ = disease_symptom_list_[0]\n",
    "    # 疾病名称增加到最后一列\n",
    "    disease_name_1.append(disease_symptom_list_[0])\n",
    "    disease_name.append(disease_name_)\n",
    "    # 住院号增加到后面\n",
    "data = DataFrame(df_list, columns=disease_symptom_attr_list)\n",
    "\n",
    "\n",
    "#插入“疾病”“年龄”和“性别”特征\n",
    "data['诊断名称'] = disease_name\n",
    "data.insert(loc=0, column='性别', value=patient_gender)\n",
    "data.insert(loc=1, column='年龄', value=patient_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将时间标准化到天\n",
    "ori_duration = []\n",
    "std_duration = []\n",
    "for column in data:\n",
    "    if '持续时间' in column:\n",
    "        for duration in data[column]:\n",
    "            if duration != 'NULL':\n",
    "                if '年' in duration:\n",
    "                    std_dura = int(duration.rstrip('年'))*365\n",
    "                if '月' in duration:\n",
    "                    std_dura = int(duration.rstrip('月'))*30\n",
    "                if '周' in duration:\n",
    "                    std_dura = int(duration.rstrip('周'))*7\n",
    "                if '天' in duration:\n",
    "                    std_dura = int(duration.rstrip('天'))\n",
    "                if '小时' in duration:\n",
    "                    std_dura = int(duration.rstrip('小时'))/24\n",
    "                if '分钟' in duration:\n",
    "                    std_dura = int(duration.rstrip('分钟'))/1440\n",
    "                ori_duration.append(duration)\n",
    "                std_duration.append(std_dura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个需要替换持续时间的字典\n",
    "std_duration_dict = {}\n",
    "std_duration_dict = dict(zip(ori_duration, std_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把所有的持续时间全部标准化\n",
    "data.replace(std_duration_dict,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续替换，NULL和“否认”全部替换为0\n",
    "data = data.replace('存在', '1').replace('否认', '0').replace('NULL', '0').replace('既往', '2').replace('可能', '4').replace('假设', '5').replace('家族', '6').replace('轻度', 1).replace('中度', 2).replace('重度', 3).replace('危重', 4).replace('牵拉样痛', '1').replace('针刺样痛', '2').replace('烧灼样痛', '3').replace('钝痛', '4').replace('绞痛', '5').replace('压榨样痛', '6').replace('痉挛样痛', '6').replace('闷痛', '7').replace('急性', '1').replace('亚急性', '1.5').replace('慢性', '2').replace('持续性', '3').replace('反复发作', '4').replace('周期性', '5').replace('间歇性', '6').replace('一过性', '7').replace('迁延不愈', '8').replace('转移性', '9').replace('晨间发作', '10').replace('夜间发作', '11').replace('午后发作', '12').replace('偶然发作', '13').replace('疾病复发', '14').replace('休息后缓解', '1').replace('活动后加剧', '2').replace('全身性', '1').replace('局域性', '2').replace('弥漫性', '3').replace('单侧', '1').replace('双侧', '2').replace('左侧', '3').replace('右侧', '4').replace('左上', '1').replace('左下', '2').replace('右上', '3').replace('右下', '4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    # data[i]=data[i].astype('int64')\n",
    "    if '持续时间' in i:\n",
    "        data[i]=data[i].astype('int64')\n",
    "    if '持续时间' not in i:\n",
    "        data[i]=data[i].astype('category')\n",
    "    if i == '年龄':\n",
    "        data[i]=data[i].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the labels into 2 objects, X and y\n",
    "X = data.drop('诊断名称',axis=1)\n",
    "Y = data['诊断名称']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train test split on the data, with the test size of 10% and a random_state of 202\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 使用index带出数据集的索引\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据进行normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a StandardScaler object to normalize the X train and test set feature data\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "print(scaled_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入LightGBM模型,训练速度更快gbdt\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "# 定义 LightGBM 模型 \n",
    "log_model = LGBMClassifier(objective='multiclass',num_class=10,verbose=0,force_col_wise=True)\n",
    "\n",
    "# 在训练集上训练LightGBM模型\n",
    "log_model.fit(scaled_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取并打印模型的所有参数\n",
    "params = log_model.get_params()\n",
    "for param, value in params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Evaluation\n",
    "\n",
    "# import evaluation metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred = log_model.predict(scaled_X_test)\n",
    "\n",
    "#confusion matrix\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、TOP1的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test,y_pred,digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对训练集群进行数据平衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "import joblib as jl\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.values.tolist()\n",
    "x = numpy.array(x)\n",
    "y = []\n",
    "for i in Y:\n",
    "    y.append(i)\n",
    "y = numpy.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保 X 和 Y 都是数值类型\n",
    "x = pd.DataFrame(x).apply(pd.to_numeric, errors='coerce')\n",
    "y = pd.Series(y).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 将数据转换为 numpy 数组\n",
    "x = X.values\n",
    "y = Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "x, y = oversample.fit_resample(x, y)\n",
    "counter = Counter(y)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(x, y, test_size=0.2, random_state=202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train_1 = scaler.fit_transform(X_train_1)\n",
    "scaled_X_test_1 = scaler.transform(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.fit(scaled_X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = log_model.predict(scaled_X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "confusion_matrix(y_test_1,y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test_1,y_pred_1,digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用平衡之后数据集的模型去诊断测试集的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取所有的json文件名\n",
    "file_path_test = '../test_data/'\n",
    "\n",
    "FilePath_list_test = []\n",
    "for root, dirs, files in os.walk(file_path_test):\n",
    "    for file in files:\n",
    "        FilePath    = os.path.join(root, file)\n",
    "        FilePath_list_test.append(FilePath)\n",
    "print(FilePath_list_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_symptom_clinical_all_test = []\n",
    "disease_Inpatient_number_all_test = []\n",
    "for file_name in FilePath_list_test:\n",
    "    with open(file_name,'r',encoding='utf-8') as load_f:\n",
    "        term_ann_list =  []\n",
    "        disease_name = file_name.split('test_data/')[1].split('_')[0]\n",
    "        Inpatient_number = file_name.split('test_data/')[1].split('_')[1].split('.json')[0]\n",
    "        disease_Inpatient_number = disease_name + '_' + Inpatient_number\n",
    "        disease_Inpatient_number_all_test.append(disease_Inpatient_number)\n",
    "        for key_d,value_d in disease_classification_dict.items():\n",
    "            if disease_name in value_d:\n",
    "                # 假设这些临床特征每个疾病都具备\n",
    "                # 后面对没有出现的特征进行特换就好了\n",
    "                load_dict = json.load(load_f)\n",
    "                annot_infos = load_dict['标注信息']\n",
    "                for annot_info in annot_infos.values():\n",
    "                    term_ann_type = annot_info[0]['术语类型']\n",
    "                    for ssu_attribute in ssu_attributes:\n",
    "                        if term_ann_type in ['症状','疾病','影像学表现'] and annot_info[1]['存在情况']['标准取值'] != '否认':\n",
    "                            term_ann_notlab = annot_info[0]['术语名称']\n",
    "                            # term_loca = annot_info[1]['发作部位']\n",
    "                            term_attr = annot_info[1][ssu_attribute]['标准取值']\n",
    "                            for key_s,value_s in term_synonym_dict.items():\n",
    "                                if term_ann_notlab in value_s:\n",
    "                                    term_ann_list.append([key_s+'_'+ssu_attribute,term_attr])\n",
    "\n",
    "\n",
    "                        if term_ann_type == '定量定性检查':\n",
    "                            term_ann_lab =  annot_info[0]['术语名称'] + annot_info[1]['结果判读']['标准取值']\n",
    "                            term_attr_lab = annot_info[1][ssu_attribute]['标准取值']\n",
    "                            for key_s,value_s in term_synonym_dict.items():\n",
    "                                if term_ann_lab in value_s:\n",
    "                                    term_ann_list.append([key_s+'_'+ssu_attribute,term_attr_lab])\n",
    "\n",
    "                        if term_ann_type in ['药物','手术'] and annot_info[1]['存在情况']['标准取值'] != '否认':\n",
    "                            term_ann_treat =  annot_info[0]['术语名称']\n",
    "                            term_treat_attr = annot_info[1]['存在情况']['标准取值']\n",
    "                            for key_s,value_s in term_synonym_dict.items():\n",
    "                                if term_ann_treat in value_s:\n",
    "                                    term_ann_list.append([key_s+'_'+'存在情况',term_treat_attr])\n",
    "\n",
    "\n",
    "                # 因为会有相同症状的不同属性取值，这里只选择出现第一次的,这里没有重复值可以写入字典\n",
    "                list_fir_symptom = []\n",
    "                term_ann_first_dict = {}\n",
    "                for i in range(len(term_ann_list)):\n",
    "                    if term_ann_list[i][0] not in list_fir_symptom:\n",
    "                        # print(term_ann_list[i])\n",
    "                        list_fir_symptom.append(term_ann_list[i][0])\n",
    "                        term_ann_first_dict[term_ann_list[i][0]] = term_ann_list[i][1]\n",
    "                # print(term_ann_first_dict)\n",
    "\n",
    "                res = []\n",
    "                for x in disease_symptom_attr_list:\n",
    "                    if x in term_ann_first_dict.keys():\n",
    "                        res.append(term_ann_first_dict[x])\n",
    "                    else:\n",
    "                        res.append('NULL')\n",
    "                disease_symptom_clinical_all_test.append([key_d,res])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_age_test = []\n",
    "patient_gender_test = []\n",
    "for file_name in FilePath_list_test:\n",
    "    with open(file_name,'r',encoding='utf-8') as load_f:\n",
    "        load_dict = json.load(load_f)\n",
    "        patient_age_gender =load_dict['原文信息'].split('。')[0].split('岁,')\n",
    "\n",
    "        patient_age_test.append(int(patient_age_gender[0]))\n",
    "        patient_gender_test.append(patient_age_gender[1].replace('男','1').replace('女','0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建完整的dataframe\n",
    "df_list_test = []\n",
    "disease_name_test = []\n",
    "disease_name_1_test = []\n",
    "for disease_symptom_list_ in disease_symptom_clinical_all_test:\n",
    "    df_list_test.append(disease_symptom_list_[1])\n",
    "    disease_name_ = disease_symptom_list_[0]\n",
    "    # 疾病名称增加到最后一列\n",
    "    disease_name_1.append(disease_symptom_list_[0])\n",
    "    disease_name_test.append(disease_name_)\n",
    "    # 住院号增加到后面\n",
    "# 将这些症状生成dataframe的形式矩阵\n",
    "data_test = DataFrame(df_list_test, columns=disease_symptom_attr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#插入“疾病”“年龄”和“性别”特征\n",
    "data_test['诊断名称'] = disease_name_test\n",
    "data_test.insert(loc=0, column='性别', value=patient_gender_test)\n",
    "data_test.insert(loc=1, column='年龄', value=patient_age_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将时间标准化到天\n",
    "ori_duration_test = []\n",
    "std_duration_test = []\n",
    "for column in data_test:\n",
    "    if '持续时间' in column:\n",
    "        for duration in data_test[column]:\n",
    "            if duration != 'NULL':\n",
    "                if '年' in duration:\n",
    "                    std_dura = int(duration.rstrip('年'))*365\n",
    "                if '月' in duration:\n",
    "                    std_dura = int(duration.rstrip('月'))*30\n",
    "                if '周' in duration:\n",
    "                    std_dura = int(duration.rstrip('周'))*7\n",
    "                if '天' in duration:\n",
    "                    std_dura = int(duration.rstrip('天'))\n",
    "                if '小时' in duration:\n",
    "                    std_dura = int(duration.rstrip('小时'))/24\n",
    "                if '分钟' in duration:\n",
    "                    std_dura = int(duration.rstrip('分钟'))/1440\n",
    "                ori_duration_test.append(duration)\n",
    "                std_duration_test.append(std_dura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个需要替换持续时间的字典\n",
    "std_duration_dict_test = {}\n",
    "std_duration_dict_test = dict(zip(ori_duration_test, std_duration_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把所有的持续时间全部标准化\n",
    "data_test.replace(std_duration_dict_test,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.replace('存在', '1').replace('否认', '0').replace('NULL', '0').replace('既往', '2').replace('可能', '4').replace('假设', '5').replace('家族', '6').replace('轻度', 1).replace('中度', 2).replace('重度', 3).replace('危重', 4).replace('牵拉样痛', '1').replace('针刺样痛', '2').replace('烧灼样痛', '3').replace('钝痛', '4').replace('绞痛', '5').replace('压榨样痛', '6').replace('痉挛样痛', '6').replace('急性', '1').replace('亚急性', '1.5').replace('慢性', '2').replace('持续性', '3').replace('反复发作', '4').replace('周期性', '5').replace('间歇性', '6').replace('一过性', '7').replace('迁延不愈', '8').replace('转移性', '9').replace('晨间发作', '10').replace('夜间发作', '11').replace('午后发作', '12').replace('偶然发作', '13').replace('疾病复发', '14').replace('休息后缓解', '1').replace('活动后加剧', '2').replace('全身性', '1').replace('局域性', '2').replace('弥漫性', '3').replace('单侧', '1').replace('双侧', '2').replace('左侧', '3').replace('右侧', '4').replace('左上', '1').replace('左下', '2').replace('右上', '3').replace('右下', '4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the labels into 2 objects, X and y\n",
    "X_test_2 = data_test.drop('诊断名称',axis=1)\n",
    "Y_test_2 = data_test['诊断名称']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a StandardScaler object to normalize the X train and test set feature data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train_1 = scaler.fit_transform(X_train_1)\n",
    "scaled_X_test_2 = scaler.transform(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = log_model.predict_proba(scaled_X_test_2)\n",
    "for y_prob in y_pred_prob:\n",
    "    diagnose_class = log_model.classes_\n",
    "    y_prob_top3 = y_prob.argsort()[-3:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = log_model.predict(scaled_X_test_2)\n",
    "\n",
    "#confusion matrix\n",
    "confusion_matrix(Y_test_2,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(classification_report(Y_test_2,y_pred_test,digits=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top3计算，用predict_proba计算出所有疾病的概率可能性，选择prob最大的前三诊断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = log_model.predict_proba(scaled_X_test_2)\n",
    "y_pred_top3_all = []\n",
    "for y_prob in y_pred_prob:\n",
    "    diagnose_class = log_model.classes_\n",
    "    y_prob_top3 = y_prob.argsort()[-3:][::-1]\n",
    "    y_pred_top3_all.append([diagnose_class[y_prob_top3[0]],diagnose_class[y_prob_top3[1]],diagnose_class[y_prob_top3[2]]])\n",
    "    # print(diagnose_class[y_prob_top3[0]],diagnose_class[y_prob_top3[1]],diagnose_class[y_prob_top3[2]])\n",
    "y_pred_top3_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_top3 = []\n",
    "for i in range(len(Y_test_2)):\n",
    "    if Y_test_2[i] in y_pred_top3_all[i]:\n",
    "        ypred_top3.append(Y_test_2[i])\n",
    "    else:\n",
    "        ypred_top3.append(y_pred_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(classification_report(Y_test_2,ypred_top3,digits=3))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a08a8d623a8732aed0b021cef2da86096866bb9674aab4c9f4232c386ffca2e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
